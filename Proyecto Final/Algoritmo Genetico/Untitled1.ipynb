{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fifty-milan",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importamos las librerias a utilizar para el algoritmo genetico\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random as rn\n",
    "\n",
    "\n",
    "from deap import base, creator, tools, algorithms\n",
    "from scipy.stats import randint\n",
    "import math\n",
    "import time\n",
    "import pickle\n",
    "import random\n",
    "import operator\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.compat.v1.keras.layers import CuDNNLSTM\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "#Esta funcion resetea las semillas de las librerias\n",
    "sd = 7 # Here sd means seed.\n",
    "def reset_random_seeds():\n",
    "  os.environ['PYTHONHASHSEED']=str(sd)\n",
    "  np.random.seed(sd)\n",
    "  rn.seed(sd)\n",
    "  tf.random.set_seed(sd)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "immune-relaxation",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_size = 24\n",
    "\n",
    "#Esta funcion recibe como dato un dataset en formato .csv y configura ciertos parametros\n",
    "def load_dataset(name=''):\n",
    "    dataframe = pd.read_csv(name, sep=\",\",index_col=False)\n",
    "    #dataframe = pd.read_csv(name)\n",
    "    #dataframe.set_index('date', inplace=True)\n",
    "    print('Features:', [i for i in dataframe.columns])\n",
    "    print('Range: ', dataframe.index[0],\" ~ \",dataframe.index[-1])\n",
    "    return dataframe\n",
    "\n",
    "#Esta funcion extrae los datos del dataset para preparar el entrenamiento\n",
    "def sel_scal_dataset(dataset, features, num_pred=1):\n",
    "    #Aqui pasamos todos los datos del dataset a numpy\n",
    "    n = len(features)\n",
    "    print(\"Longitud del dataset:\", len(dataset))\n",
    "    dataset = dataset[features].to_numpy()\n",
    "    dataset = dataset.astype('float32')\n",
    "\n",
    "    # normalize the dataset\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    dataset = scaler.fit_transform(dataset)\n",
    "    num_samples = len(dataset)/num_pred\n",
    "    test_size = int(num_samples * 0.1)*num_pred\n",
    "    num_samples = (len(dataset) - test_size)/num_pred\n",
    "    train_size = int(num_samples*0.75)*num_pred\n",
    "    valid_size =  (len(dataset) - test_size) - train_size\n",
    "    #train es el conjunto de entrenamiento\n",
    "    #valid es el conjunto de validacion\n",
    "    #test es el conjunto de prueba\n",
    "    train, valid, test = dataset[0:train_size, :], dataset[train_size:train_size+valid_size, :], dataset[train_size+valid_size:len(dataset), :]\n",
    "    \n",
    "    print(\"Elementos del conjunto de entrenamiento: {}\".format(train_size))\n",
    "    print(\"Elementos del conjunto de validacion: {}\".format(valid_size))\n",
    "    print(\"Elementos del conjunto de prueba: {}\".format(test_size))\n",
    "\n",
    "    return train, valid, test, scaler, n\n",
    "\n",
    "#Esta funcion prepara los datos del dataset de tal forma a poder trabajar directamente con los vectores X e Y\n",
    "def prepare_dataset(dataset, n, window_size=1, window_pred=1, step=1, pred_24=False):\n",
    "    dataX, dataY = [], []\n",
    "    offset = 0\n",
    "    if pred_24:\n",
    "      while((offset+window_size)%24 != 0):\n",
    "        offset += 1\n",
    "    else:\n",
    "      step = 1 # Verificar\n",
    "    \n",
    "    if n > 1:\n",
    "      #Esta es una forma de pones desde i=offset, hasta i=(len(dataset) - window_size - window_pred) con un paso de step\n",
    "      for i in range(offset, len(dataset) - window_size - window_pred, step):\n",
    "        aux_after_window = []\n",
    "        window = dataset[i:(i + window_size)]\n",
    "        j = i\n",
    "        aux_after_window = [[j] for j in dataset[(j + window_size):(j + window_size + window_pred), 0]]\n",
    "        after_window = aux_after_window\n",
    "        dataX.append(window)\n",
    "        dataY.append(after_window)\n",
    "    \n",
    "    else:\n",
    "      for i in range(offset, len(dataset) - window_size - window_pred, step):\n",
    "        window = dataset[i:(i + window_size)]\n",
    "        after_window = dataset[(i + window_size):(i + window_size + window_pred)]\n",
    "        dataX.append(window)\n",
    "        dataY.append(after_window)\n",
    "\n",
    "    return np.asarray(dataX), np.asarray(dataY)\n",
    "\n",
    "#\n",
    "def evaluationLSTM(chromosome):\n",
    "  eval_look_back = chromosome[0]\n",
    "  eval_num_units = chromosome[1]\n",
    "  eval_neurons_1 = chromosome[2]\n",
    "\n",
    "  trainX, trainY = prepare_dataset(train, n, eval_look_back, pred, step=24, pred_24=True)\n",
    "  validX, validY = prepare_dataset(valid, n, eval_look_back, pred, step=24, pred_24=True)\n",
    "  testX, testY = prepare_dataset(test, n, eval_look_back, pred, step=24, pred_24=True)\n",
    "  \n",
    "  reset_random_seeds()\n",
    "  model = Sequential([CuDNNLSTM(eval_num_units, input_shape=(eval_look_back, n)),\n",
    "                        Dense(eval_neurons_1, activation='relu'),\n",
    "                        Dense(pred_size, activation='linear')])\n",
    "\n",
    "  model.compile(loss=\"mean_squared_error\", optimizer=\"Adam\")\n",
    "\n",
    "  start = time.time()\n",
    "  hist = model.fit(trainX, trainY, epochs=100, shuffle=True, batch_size=50, validation_data=(validX, validY),\n",
    "                     callbacks=[EarlyStopping(monitor='val_loss', patience=30)], verbose=0)\n",
    "\n",
    "  end = time.time()\n",
    "  print('Time training:', end-start)\n",
    "\n",
    "  model.summary()\n",
    "  testPredict = model.predict(testX)\n",
    "  # Calculate the RMSE score as fitness score for GA\n",
    "  rmse = np.sqrt(mean_squared_error(testY[:, :, 0], testPredict))\n",
    "  print('Validation RMSE: ', rmse,'\\n')\n",
    "  return rmse, \n",
    "\n",
    "\n",
    "def initIndividual(min_max_list):\n",
    "  individual = []\n",
    "  for i in min_max_list:\n",
    "    individual.append(randint.rvs(i[0], i[1]))\n",
    "\n",
    "  return individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "silver-enlargement",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-bbced7669aac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mphysical_devices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlist_physical_devices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'GPU'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_memory_growth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphysical_devices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "interpreted-radio",
   "metadata": {},
   "outputs": [],
   "source": [
    "def geneticAlgorithm(checkpoint=None):\n",
    "    best_ind = None\n",
    "    best_fit = None\n",
    "    if checkpoint:\n",
    "        with open(checkpoint, \"rb\") as cp_file:\n",
    "            cp = pickle.load(cp_file)\n",
    "        population = cp[\"population\"]\n",
    "        start_gen = cp[\"generation\"]\n",
    "        halloffame = cp[\"halloffame\"]\n",
    "        logbook = cp[\"logbook\"]\n",
    "        random.setstate(cp[\"rndstate\"])\n",
    "    \n",
    "    else:\n",
    "        population = toolbox.population(n=population_size)\n",
    "        start_gen = 0\n",
    "        halloffame = tools.HallOfFame(maxsize=10)\n",
    "        logbook = tools.Logbook()\n",
    "\n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats.register(\"avg\", np.mean)\n",
    "    stats.register(\"std\", np.std)\n",
    "    stats.register(\"min\", np.min)\n",
    "    stats.register(\"max\", np.max)\n",
    "\n",
    "    for gen in range(start_gen, num_generations):\n",
    "        population = algorithms.varAnd(population, toolbox, cxpb=CXPB, mutpb=MUTPB)\n",
    "\n",
    "        # Evaluate the individuals with an invalid fitness\n",
    "        invalid_ind = [ind for ind in population if not ind.fitness.valid]\n",
    "        fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "        \n",
    "        halloffame.update(population)\n",
    "        record = stats.compile(population)\n",
    "            \n",
    "        best_ind = None\n",
    "        best_fit = None\n",
    "        \n",
    "        for ind in population:\n",
    "            if best_ind is not None:\n",
    "                if operator.le(ind.fitness.values[0], best_fit):\n",
    "                    best_ind = ind\n",
    "                    best_fit = ind.fitness.values[0]\n",
    "            \n",
    "            else:\n",
    "                if operator.lt(ind.fitness.values, halloffame[0].fitness.values):\n",
    "                    best_ind = ind\n",
    "                    best_fit = ind.fitness.values[0]\n",
    "        \n",
    "        if best_ind is None and best_fit is None:\n",
    "            best_ind = halloffame[0]\n",
    "            best_fit = halloffame[0].fitness.values[0]\n",
    "        \n",
    "        logbook.record(gen=gen, evals=len(invalid_ind), best_ind=best_ind, best_fit=best_fit, **record)\n",
    "        population = toolbox.select(population, k=len(population))\n",
    "\n",
    "        if gen % FREQ == 0:\n",
    "            cp = dict(population=population, generation=gen, halloffame=halloffame,\n",
    "                      logbook=logbook, rndstate=random.getstate())\n",
    "\n",
    "            with open(\"model_1HL_FINAL.pkl\", \"wb\") as cp_file:\n",
    "                pickle.dump(cp, cp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "talented-vatican",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-0d6a836d4cfc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m# Se crea la clase \"Individual\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mcreator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Individual\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfitness\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLSTMOptimization\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m#Se crea el \"toolbox\" para las operaciones\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\Inteligencia_Artificial\\lib\\site-packages\\deap\\creator.py\u001b[0m in \u001b[0;36mcreate\u001b[1;34m(name, base, **kargs)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;31m# Check if the base class has to be replaced\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mbase\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mclass_replacers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    153\u001b[0m         \u001b[0mbase\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclass_replacers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbase\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "gen_length = 3\n",
    "looback_size = (1, 168)\n",
    "num_units = (1, 100)\n",
    "neurons_1 = (10, 200)\n",
    "\n",
    "optimization_params = [looback_size, num_units, neurons_1]\n",
    "lower_bound = [i[0] for i in optimization_params]\n",
    "upper_bound = [i[1] for i in optimization_params]\n",
    "\n",
    "population_size = 50\n",
    "num_generations = 100\n",
    "\n",
    "CXPB = 0.3\n",
    "MUTPB = 0.2\n",
    "FREQ = 1\n",
    "\n",
    "# Se crea la clase \"LSTMOptimization\"\n",
    "creator.create(\"LSTMOptimization\", base.Fitness, weights=(-1.0, ))\n",
    "\n",
    "# Se crea la clase \"Individual\"\n",
    "creator.create(\"Individual\", list, fitness = creator.LSTMOptimization)\n",
    "\n",
    "#Se crea el \"toolbox\" para las operaciones\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"initialization\", initIndividual, optimization_params)\n",
    "toolbox.register(\"individual\", tools.initIterate, creator.Individual, toolbox.initialization)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "toolbox.register(\"mate\", tools.cxUniform, indpb=0.5) # Crossover\n",
    "toolbox.register(\"mutate\", tools.mutUniformInt, low=lower_bound, up=upper_bound, indpb=0.2) # Mutation\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3) # Selection\n",
    "toolbox.register(\"evaluate\", evaluationLSTM) # Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "nearby-alignment",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'toolbox' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-6d031ad1fe84>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgeneticAlgorithm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-3131c25be316>\u001b[0m in \u001b[0;36mgeneticAlgorithm\u001b[1;34m(checkpoint)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mpopulation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtoolbox\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpopulation_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mstart_gen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mhalloffame\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHallOfFame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaxsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'toolbox' is not defined"
     ]
    }
   ],
   "source": [
    "geneticAlgorithm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustainable-globe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
