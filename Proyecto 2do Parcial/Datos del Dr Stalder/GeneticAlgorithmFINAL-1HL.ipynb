{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "MjJ7lchsfllo",
    "outputId": "d0089103-c70d-4526-ff9e-b006db0f020e"
   },
   "outputs": [],
   "source": [
    "#!pip install deap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random as rn\n",
    "\n",
    "sd = 7 # Here sd means seed.\n",
    "\n",
    "def reset_random_seeds():\n",
    "  os.environ['PYTHONHASHSEED']=str(sd)\n",
    "  np.random.seed(sd)\n",
    "  rn.seed(sd)\n",
    "  tf.random.set_seed(sd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qSkOHXQ3fr9O"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'deap'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-58d0ef6ae677>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mdeap\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbase\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtools\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malgorithms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandint\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'deap'"
     ]
    }
   ],
   "source": [
    "from deap import base, creator, tools, algorithms\n",
    "from scipy.stats import randint\n",
    "import math\n",
    "import time\n",
    "import pickle\n",
    "import random\n",
    "import operator\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.compat.v1.keras.layers import CuDNNLSTM\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_size = 24\n",
    "\n",
    "def load_dataset(name=''):\n",
    "    dataframe = pd.read_csv(name)\n",
    "    dataframe.set_index('date', inplace=True)\n",
    "    print('Features:', [i for i in dataframe.columns])\n",
    "    print('Range: ', dataframe.index[0],\" ~ \",dataframe.index[-1])\n",
    "    return dataframe\n",
    "\n",
    "def sel_scal_dataset(dataset, features, num_pred=1):\n",
    "    n = len(features)\n",
    "    print(\"Longitud del dataset:\", len(dataset))\n",
    "    dataset = dataset[features].to_numpy()\n",
    "    dataset = dataset.astype('float32')\n",
    "\n",
    "    # normalize the dataset\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    dataset = scaler.fit_transform(dataset)\n",
    "    num_samples = len(dataset)/num_pred\n",
    "    test_size = int(num_samples * 0.1)*num_pred\n",
    "    num_samples = (len(dataset) - test_size)/num_pred\n",
    "    train_size = int(num_samples*0.75)*num_pred\n",
    "    valid_size =  (len(dataset) - test_size) - train_size\n",
    "    train, valid, test = dataset[0:train_size, :], dataset[train_size:train_size+valid_size, :], dataset[train_size+valid_size:len(dataset), :]\n",
    "    \n",
    "    print(\"Elementos del conjunto de entrenamiento: {}\".format(train_size))\n",
    "    print(\"Elementos del conjunto de validacion: {}\".format(valid_size))\n",
    "    print(\"Elementos del conjunto de prueba: {}\".format(test_size))\n",
    "\n",
    "    return train, valid, test, scaler, n\n",
    "\n",
    "def prepare_dataset(dataset, n, window_size=1, window_pred=1, step=1, pred_24=False):\n",
    "    dataX, dataY = [], []\n",
    "    offset = 0\n",
    "    if pred_24:\n",
    "      while((offset+window_size)%24 != 0):\n",
    "        offset += 1\n",
    "    else:\n",
    "      step = 1 # Verificar\n",
    "\n",
    "    if n > 1:\n",
    "      for i in range(offset, len(dataset) - window_size - window_pred, step):\n",
    "        aux_after_window = []\n",
    "        window = dataset[i:(i + window_size)]\n",
    "        j = i\n",
    "        aux_after_window = [[j] for j in dataset[(j + window_size):(j + window_size + window_pred), 0]]\n",
    "        after_window = aux_after_window\n",
    "        dataX.append(window)\n",
    "        dataY.append(after_window)\n",
    "    \n",
    "    else:\n",
    "      for i in range(offset, len(dataset) - window_size - window_pred, step):\n",
    "        window = dataset[i:(i + window_size)]\n",
    "        after_window = dataset[(i + window_size):(i + window_size + window_pred)]\n",
    "        dataX.append(window)\n",
    "        dataY.append(after_window)\n",
    "\n",
    "    return np.asarray(dataX), np.asarray(dataY)\n",
    "\n",
    "\n",
    "def evaluationLSTM(chromosome):\n",
    "  eval_look_back = chromosome[0]\n",
    "  eval_num_units = chromosome[1]\n",
    "  eval_neurons_1 = chromosome[2]\n",
    "\n",
    "  trainX, trainY = prepare_dataset(train, n, eval_look_back, pred, step=24, pred_24=True)\n",
    "  validX, validY = prepare_dataset(valid, n, eval_look_back, pred, step=24, pred_24=True)\n",
    "  testX, testY = prepare_dataset(test, n, eval_look_back, pred, step=24, pred_24=True)\n",
    "  \n",
    "  reset_random_seeds()\n",
    "  model = Sequential([CuDNNLSTM(eval_num_units, input_shape=(eval_look_back, n)),\n",
    "                        Dense(eval_neurons_1, activation='relu'),\n",
    "                        Dense(pred_size, activation='linear')])\n",
    "\n",
    "  model.compile(loss=\"mean_squared_error\", optimizer=\"Adam\")\n",
    "\n",
    "  start = time.time()\n",
    "  hist = model.fit(trainX, trainY, epochs=100, shuffle=True, batch_size=50, validation_data=(validX, validY),\n",
    "                     callbacks=[EarlyStopping(monitor='val_loss', patience=30)], verbose=0)\n",
    "\n",
    "  end = time.time()\n",
    "  print('Time training:', end-start)\n",
    "\n",
    "  model.summary()\n",
    "  testPredict = model.predict(testX)\n",
    "  # Calculate the RMSE score as fitness score for GA\n",
    "  rmse = np.sqrt(mean_squared_error(testY[:, :, 0], testPredict))\n",
    "  print('Validation RMSE: ', rmse,'\\n')\n",
    "  return rmse, \n",
    "\n",
    "def initIndividual(min_max_list):\n",
    "  individual = []\n",
    "  for i in min_max_list:\n",
    "    individual.append(randint.rvs(i[0], i[1]))\n",
    "\n",
    "  return individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "kTQMFK-ygTsL",
    "outputId": "325f450d-b8e8-4d71-85f3-248b258514e3"
   },
   "outputs": [],
   "source": [
    "pred = 24\n",
    "df = load_dataset('dataset_09-17NEW.csv')\n",
    "df[\"Weekday\"] = (df[\"Weekday Name\"] > 5).astype(int)\n",
    "df[\"SIN_past\"] = df[\"SIN\"].shift(periods=(pred*7))\n",
    "df[\"temp_t1\"] = df[\"Temperatura.Asuncion\"].shift(periods=-pred)\n",
    "df[\"Year_t1\"] = df[\"Year\"].shift(periods=-pred)\n",
    "df[\"Hour_t1\"] = df[\"Hour\"].shift(periods=-pred)\n",
    "df[\"Month_t1\"] = df[\"Month\"].shift(periods=-pred)\n",
    "df[\"Weekday_t1\"] = df[\"Weekday\"].shift(periods=-pred)\n",
    "df = df.dropna()\n",
    "\n",
    "train, valid, test, scaler, n = sel_scal_dataset(df[\"2009\":\"2014\"], [\"SIN\", \"Temperatura.Asuncion\", \"Year\", \"Weekday\", \"Month\",\n",
    "                                                                     \"SIN_past\", \"temp_t1\", \"Year_t1\", \"Weekday_t1\", \"Month_t1\"], pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "6p5p0G0AiKZN",
    "outputId": "2a854403-ce49-44b8-c432-f3c6033d4564"
   },
   "outputs": [],
   "source": [
    "gen_length = 3\n",
    "looback_size = (1, 168)\n",
    "num_units = (1, 100)\n",
    "neurons_1 = (10, 200)\n",
    "\n",
    "optimization_params = [looback_size, num_units, neurons_1]\n",
    "lower_bound = [i[0] for i in optimization_params]\n",
    "upper_bound = [i[1] for i in optimization_params]\n",
    "\n",
    "population_size = 50\n",
    "num_generations = 100\n",
    "\n",
    "CXPB = 0.3\n",
    "MUTPB = 0.2\n",
    "FREQ = 1\n",
    "\n",
    "# Se crea la clase \"LSTMOptimization\"\n",
    "creator.create(\"LSTMOptimization\", base.Fitness, weights=(-1.0, ))\n",
    "\n",
    "# Se crea la clase \"Individual\"\n",
    "creator.create(\"Individual\", list, fitness = creator.LSTMOptimization)\n",
    "\n",
    "#Se crea el \"toolbox\" para las operaciones\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"initialization\", initIndividual, optimization_params)\n",
    "toolbox.register(\"individual\", tools.initIterate, creator.Individual, toolbox.initialization)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "toolbox.register(\"mate\", tools.cxUniform, indpb=0.5) # Crossover\n",
    "toolbox.register(\"mutate\", tools.mutUniformInt, low=lower_bound, up=upper_bound, indpb=0.2) # Mutation\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3) # Selection\n",
    "toolbox.register(\"evaluate\", evaluationLSTM) # Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "-wmuPBeD7bBZ",
    "outputId": "f098df97-20ae-4120-8219-7d574f0150c5"
   },
   "outputs": [],
   "source": [
    "def geneticAlgorithm(checkpoint=None):\n",
    "    best_ind = None\n",
    "    best_fit = None\n",
    "    if checkpoint:\n",
    "        with open(checkpoint, \"rb\") as cp_file:\n",
    "            cp = pickle.load(cp_file)\n",
    "        population = cp[\"population\"]\n",
    "        start_gen = cp[\"generation\"]\n",
    "        halloffame = cp[\"halloffame\"]\n",
    "        logbook = cp[\"logbook\"]\n",
    "        random.setstate(cp[\"rndstate\"])\n",
    "    \n",
    "    else:\n",
    "        population = toolbox.population(n=population_size)\n",
    "        start_gen = 0\n",
    "        halloffame = tools.HallOfFame(maxsize=10)\n",
    "        logbook = tools.Logbook()\n",
    "\n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats.register(\"avg\", np.mean)\n",
    "    stats.register(\"std\", np.std)\n",
    "    stats.register(\"min\", np.min)\n",
    "    stats.register(\"max\", np.max)\n",
    "\n",
    "    for gen in range(start_gen, num_generations):\n",
    "        population = algorithms.varAnd(population, toolbox, cxpb=CXPB, mutpb=MUTPB)\n",
    "\n",
    "        # Evaluate the individuals with an invalid fitness\n",
    "        invalid_ind = [ind for ind in population if not ind.fitness.valid]\n",
    "        fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "        \n",
    "        halloffame.update(population)\n",
    "        record = stats.compile(population)\n",
    "            \n",
    "        best_ind = None\n",
    "        best_fit = None\n",
    "        \n",
    "        for ind in population:\n",
    "            if best_ind is not None:\n",
    "                if operator.le(ind.fitness.values[0], best_fit):\n",
    "                    best_ind = ind\n",
    "                    best_fit = ind.fitness.values[0]\n",
    "            \n",
    "            else:\n",
    "                if operator.lt(ind.fitness.values, halloffame[0].fitness.values):\n",
    "                    best_ind = ind\n",
    "                    best_fit = ind.fitness.values[0]\n",
    "        \n",
    "        if best_ind is None and best_fit is None:\n",
    "            best_ind = halloffame[0]\n",
    "            best_fit = halloffame[0].fitness.values[0]\n",
    "        \n",
    "        logbook.record(gen=gen, evals=len(invalid_ind), best_ind=best_ind, best_fit=best_fit, **record)\n",
    "        population = toolbox.select(population, k=len(population))\n",
    "\n",
    "        if gen % FREQ == 0:\n",
    "            cp = dict(population=population, generation=gen, halloffame=halloffame,\n",
    "                      logbook=logbook, rndstate=random.getstate())\n",
    "\n",
    "            with open(\"model_1HL_FINAL.pkl\", \"wb\") as cp_file:\n",
    "                pickle.dump(cp, cp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "XoedL1Gyqrvf",
    "outputId": "443f0555-1380-43ca-9cbf-937f14222b52"
   },
   "outputs": [],
   "source": [
    "geneticAlgorithm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ykpYLdmUrnr-"
   },
   "outputs": [],
   "source": [
    "file = open('model_2HLV1.pkl', 'rb')\n",
    "\n",
    "# dump information to that file\n",
    "data = pickle.load(file)\n",
    "\n",
    "# close the file\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "3KCZKnwfso98",
    "outputId": "042c5452-80e0-4837-fb84-7c22a1334297"
   },
   "outputs": [],
   "source": [
    "data['halloffame'].items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G3MXRFOY3VSP"
   },
   "outputs": [],
   "source": [
    "data['halloffame'].keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "GeneticAlgorithm.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
